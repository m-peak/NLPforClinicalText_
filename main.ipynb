{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1. PREPROCESS\n",
    "# 1-1. Setup & Load Data\n",
    "#########################\n",
    "'''\n",
    "# Module Installation\n",
    "!{sys.executable} -m pip install pandas\n",
    "'''\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SEED = 1234\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# 1-1-1.Define Data Path\n",
    "DATA_PATH = 'mimic-iii-1.4'\n",
    "assert os.path.isdir(DATA_PATH)\n",
    "DATA_FILE = 'notes_labeled.csv'\n",
    "\n",
    "notes = pd.read_csv('%s/%s' % (DATA_PATH, 'NOTEEVENTS.csv'), usecols=['HADM_ID', 'CATEGORY', 'TEXT'], dtype={'CATEGORY': str, 'TEXT': str})\n",
    "procodes = pd.read_csv('%s/%s' % (DATA_PATH, 'PROCEDURES_ICD.csv'), usecols=['HADM_ID', 'ICD9_CODE'], dtype={'HADM_ID': np.uint64, 'ICD9_CODE': str})\n",
    "digcodes = pd.read_csv('%s/%s' % (DATA_PATH, 'DIAGNOSES_ICD.csv'), usecols=['HADM_ID', 'ICD9_CODE'], dtype={'HADM_ID': np.uint64, 'ICD9_CODE': str})\n",
    "pcodes = procodes.copy()\n",
    "dcodes = digcodes.copy()\n",
    "pcodes['ICD9_CODE'] = pcodes['ICD9_CODE'].apply(lambda x: x[:2] + '.' + x[2:] if len(str(x)) > 2 else x) # decimal point between the 2rd and 3rd digit\n",
    "dcodes['ICD9_CODE'] = dcodes['ICD9_CODE'].apply(lambda x: x[:3] + '.' + x[3:] if len(str(x)) > 3 else x) # decimal point between the 3rd and 4th digit\n",
    "codes = pd.concat([pcodes, dcodes])\n",
    "codes.to_csv('%s/%s' % (DATA_PATH, 'codes.csv'), index=False, columns=['HADM_ID', 'ICD9_CODE'], header=['HADM_ID', 'ICD9_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-hardware",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1-2. Preprocess Data\n",
    "#########################\n",
    "import string\n",
    "import csv\n",
    "\n",
    "# 1-2-1.Preprocess Clinical Notes\n",
    "def preprocess_text(df):\n",
    "    # Discharge Summary Notes\n",
    "    df.drop(df[df['CATEGORY']!='Discharge summary'].index, inplace=True)\n",
    "    df.drop(columns=['CATEGORY'], inplace=True)\n",
    "    df.TEXT = df.TEXT.fillna(' ') # replace NA with space\n",
    "    df.TEXT = df.TEXT.str.replace('\\n', ' ') # replace newline with space\n",
    "    df.TEXT = df.TEXT.str.replace('\\r', ' ') # replace return with space\n",
    "    \n",
    "    # Replace punctuation (special characters) with space\n",
    "    dictionary = dict.fromkeys(string.punctuation, ' ') # e.g. {'=' : ' '}\n",
    "    trans_dictionary = str.maketrans(dictionary) # dictionary for translation \n",
    "    df.TEXT = df.TEXT.str.translate(trans_dictionary) # replace punctuation with space\n",
    "    df.TEXT = df.TEXT.str.replace('\\d+', ' ') # replace digit with space\n",
    "    df.TEXT = df.TEXT.str.lower() # lowercase\n",
    "    df.TEXT = df.TEXT.str.replace('\\s+', ' ', regex=True) # replace multiple spaces with space\n",
    "    return df\n",
    "\n",
    "# 1-2-2.Concat Notes\n",
    "def concat_notes(df):\n",
    "    df = df.sort_values(['HADM_ID'], ascending=True)\n",
    "    df0 = pd.DataFrame(columns=['HADM_ID', 'TEXT'])\n",
    "    txs = []\n",
    "    hadm_id = df.iloc[0]['HADM_ID']\n",
    "    for row in df.itertuples():\n",
    "        id = row.HADM_ID\n",
    "        if hadm_id != id:\n",
    "            df0 = df0.append({'HADM_ID': int(hadm_id), 'TEXT': str(' '.join(txs))}, ignore_index=True)\n",
    "            txs = []\n",
    "            hadm_id = id\n",
    "        txs.append(str(row.TEXT))\n",
    "    df0 = df0.append({'HADM_ID': int(hadm_id), 'TEXT': str(' '.join(txs))}, ignore_index=True)\n",
    "    return df0\n",
    "\n",
    "# 1-2-3.Exclude Unnecessary Codes (Consolidate Labels)\n",
    "def exclude_codes(df1, df2):\n",
    "    ids = set(df1['HADM_ID']) # = df1['HADM_ID'].unique()\n",
    "    idcs = []\n",
    "    i = 0\n",
    "    for id in df2['HADM_ID']: # = df2.HADM_ID.values\n",
    "        if id in ids: # or not in ids\n",
    "            idcs.append(i)\n",
    "        i += 1\n",
    "    return df2.iloc[idcs] # or df2.drop(idcs, inplace=True)\n",
    "\n",
    "# 1-2-4.Concat Codes\n",
    "def concat_codes(df):\n",
    "    df = df.sort_values(['HADM_ID'], ascending=True)\n",
    "    df0 = pd.DataFrame(columns=['HADM_ID', 'ICD9_CODE'])\n",
    "    cds = []\n",
    "    hadm_id = df.iloc[0]['HADM_ID']\n",
    "    for row in df.itertuples():\n",
    "        id = row.HADM_ID\n",
    "        if hadm_id != id:\n",
    "            df0 = df0.append({'HADM_ID': int(hadm_id), 'ICD9_CODE': str(';'.join(cds))}, ignore_index=True)\n",
    "            cds = []\n",
    "            hadm_id = id\n",
    "        cds.append(str(row.ICD9_CODE))\n",
    "    df0 = df0.append({'HADM_ID': int(hadm_id), 'ICD9_CODE': str(';'.join(cds))}, ignore_index=True)\n",
    "    return df0\n",
    "\n",
    "# 1-2-5.Merge & Save (notes_labeled.csv)\n",
    "def merge_save(df1, df2, file_name):\n",
    "    df1 = df1.sort_values(['HADM_ID'])\n",
    "    df2 = df2.sort_values(['HADM_ID'])\n",
    "    df = pd.merge(df1, df2, how='inner', on='HADM_ID')\n",
    "    df['LENGTH'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['LENGTH'], ascending=False) # descending order\n",
    "    df.to_csv('%s/%s' % (DATA_PATH, file_name), columns=['HADM_ID', 'LENGTH', 'TEXT', 'ICD9_CODE'],\n",
    "              index=False, header=True, quoting=csv.QUOTE_NONE, escapechar='')\n",
    "    return df\n",
    "\n",
    "# Preprocess Data\n",
    "notes = preprocess_text(notes)\n",
    "notes = concat_notes(notes)\n",
    "codes = exclude_codes(notes, codes)\n",
    "codes = concat_codes(codes)\n",
    "notes['HADM_ID'] = notes['HADM_ID'].astype('int')\n",
    "notes_labeled = merge_save(notes, codes, DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-projector",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1-3. Top ICD9_CODE\n",
    "#########################\n",
    "from collections import Counter\n",
    "TOP = 50\n",
    "\n",
    "# 1-3-1.Top ICD9_CODE\n",
    "def get_top_codes(df, top=TOP):\n",
    "    counts = Counter()\n",
    "    for row in df.itertuples():\n",
    "        for code in str(row.ICD9_CODE).split(';'):\n",
    "            counts[code] += 1\n",
    "    codes = counts.most_common(top) # code, not count\n",
    "    return [i[0] for i in codes]\n",
    "\n",
    "# 1-3-2.Save Top Codes\n",
    "def save_top_codes(codes):\n",
    "    df = pd.DataFrame(codes, columns=['ICD9_CODE'])\n",
    "    df.to_csv('%s/codes_top.csv' % DATA_PATH, columns=['ICD9_CODE'], index=False, header=False, quoting=csv.QUOTE_NONE, escapechar='')\n",
    "    return df\n",
    "\n",
    "# Top ICD9_CODE\n",
    "codes_top = get_top_codes(notes_labeled)\n",
    "codes_top = save_top_codes(codes_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1-4. Word2Vec Model\n",
    "#########################\n",
    "'''\n",
    "# Module Installation\n",
    "!{sys.executable} -m pip install gensim\n",
    "'''\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1-4-1.Word2Vec for Word Embedding based on the frequency of occurrence of a word\n",
    "# similar words have vectors near each other (relationships = distance in vector space)\n",
    "def create_w2v(text, vec_size=100, window=5, min_count=3, workers=4): # Default: vec_size=100, window=5\n",
    "    # Word2Vec default: sg=0:CBOW(ContinuousBagOfWords)(sg=1:Skip-gram), vector_size=100, min_count=5, workers=3, epochs=5\n",
    "    w2v = Word2Vec(sentences=text, size=vec_size, min_count=min_count, workers=workers)\n",
    "    return w2v\n",
    "\n",
    "# 1-4-2.Tokenize Text into Words on notes_labeled\n",
    "text = [vals.split() for vals in notes_labeled.TEXT.values]\n",
    "\n",
    "# W2V on text of notes_labeled\n",
    "w2v = create_w2v(text)\n",
    "w2v.save('%s/%s' % (DATA_PATH, 'w2v.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1-5. Vocabulary\n",
    "#########################\n",
    "# 1-5-1.Get Vocab List\n",
    "def get_vocab_list(vocab):\n",
    "    vocab_list = list(vocab)\n",
    "    vocab_df = pd.DataFrame(vocab_list)\n",
    "    vocab_df.to_csv('%s/vocab.csv' % DATA_PATH, index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "    return vocab_list\n",
    "\n",
    "# Vocabulary\n",
    "vocab = w2v.wv.vocab # datatype = dictionary\n",
    "del w2v # free up memory\n",
    "vocab_list = get_vocab_list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1-6. Top ICD9_CODE Text\n",
    "#########################\n",
    "MAX_LENGTH = 2500\n",
    "\n",
    "# 1-6-1.Get Max Length Text\n",
    "def get_max_length_text(text):\n",
    "    text = np.array(text)\n",
    "    idxs = []\n",
    "    done = False\n",
    "    for v in vocab_list:\n",
    "        for i, w in enumerate(text):\n",
    "            if len(idxs) < MAX_LENGTH:\n",
    "                idxs.append(i)\n",
    "            else:\n",
    "                done = True\n",
    "                break\n",
    "        if done:\n",
    "            break\n",
    "    return list(text[sorted(idxs)])\n",
    "\n",
    "# 1-6-2.Get Top notes_labeled\n",
    "def get_notes_labeled_top(df, codes_top, file_name):\n",
    "    codes_top = set(codes_top)\n",
    "    cols = ['HADM_ID', 'LENGTH', 'TEXT', 'ICD9_CODE']\n",
    "    df0 = pd.DataFrame(columns=cols)\n",
    "    for row in df.itertuples():\n",
    "        codes = set(str(row.ICD9_CODE).split(';'))\n",
    "        cds = codes.intersection(codes_top)\n",
    "        if len(cds) > 0:\n",
    "            length = int(row.LENGTH)\n",
    "            text = str(row.TEXT).split()\n",
    "            if length > MAX_LENGTH:\n",
    "                text = get_max_length_text(text)\n",
    "                length = len(text)\n",
    "            df0 = df0.append({'HADM_ID': int(row.HADM_ID), 'LENGTH': int(length), 'TEXT': str(' '.join(text)), 'ICD9_CODE': str(';'.join(cds))}, ignore_index=True)\n",
    "\n",
    "    df0 = df0.sort_values(['LENGTH'], ascending=False) # descending order\n",
    "    df0.to_csv('%s/%s' % (DATA_PATH, file_name), columns=cols,\n",
    "               index=False, header=True, quoting=csv.QUOTE_NONE, escapechar='')\n",
    "    return df0\n",
    "\n",
    "# Top notes_labeled\n",
    "notes_labeled_top = get_notes_labeled_top(notes_labeled, codes_top['ICD9_CODE'], 'notes_labeled_top.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1-7. Split Data for TOP\n",
    "#########################\n",
    "import random\n",
    "\n",
    "# 1-7-1.Split Data of top_notes_labeled\n",
    "def split_data(df, train_name='train_top.csv', valid_name='valid_top.csv', test_name='test_top.csv', ratio1=0.9, ratio2=0.7):\n",
    "    cols = ['HADM_ID', 'LENGTH', 'TEXT', 'ICD9_CODE']\n",
    "    max_length = len(df)\n",
    "    train_idx = int(max_length * ratio1)\n",
    "    valid_idx = train_idx + int((max_length - train_idx) * ratio2)\n",
    "    \n",
    "    # Reproduce the same train/valid/test datasets and the results\n",
    "    random_indices_file = '%s/%s' % (DATA_PATH, 'random_indices.csv')\n",
    "    if os.path.isfile(random_indices_file):\n",
    "        idxs = (pd.read_csv(random_indices_file, header=None, dtype=np.uint64)[0]).tolist()\n",
    "    else:\n",
    "        idxs = [i for i in range(max_length)]\n",
    "        random.shuffle(idxs)\n",
    "        df_idxs = pd.DataFrame(idxs)\n",
    "        df_idxs.to_csv(random_indices_file, index=False, header=False, quoting=csv.QUOTE_NONE, escapechar='')\n",
    "    \n",
    "    trn, vld, tst = df.iloc[idxs[:train_idx]], df.iloc[idxs[train_idx:valid_idx]], df.iloc[idxs[valid_idx:max_length]]\n",
    "    trn.to_csv('%s/%s' % (DATA_PATH, train_name), index=False, columns=cols)\n",
    "    vld.to_csv('%s/%s' % (DATA_PATH, valid_name), index=False, columns=cols)\n",
    "    tst.to_csv('%s/%s' % (DATA_PATH, test_name), index=False, columns=cols)\n",
    "    return trn, vld, tst\n",
    "\n",
    "# Split Data on top_notes_labeled\n",
    "train_top, valid_top, test_top = split_data(notes_labeled_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 2. W2V EMBEDDING MATRIX\n",
    "# 2-1. W2V Matrix\n",
    "#########################\n",
    "PAD_CHAR = '*'\n",
    "\n",
    "def create_embeddings(wv_file, out_file):\n",
    "    model = Word2Vec.load(wv_file)\n",
    "    wv = model.wv\n",
    "    del model # free up memory\n",
    "    vocab = set(wv.vocab)\n",
    "    ind2w = {i+1:w for i, w in enumerate(sorted(vocab))} # e.g. {4: 'aabdominal', 5: 'aabsent'}\n",
    "    Ws, words = build_matrix(ind2w, wv)\n",
    "    save_embeddings(Ws, words, out_file)\n",
    "\n",
    "def build_matrix(ind2w, wv):\n",
    "    length = len(wv.word_vec(wv.index2word[0])) # 100\n",
    "    Ws = np.zeros((len(ind2w)+1, length)) # shape=(52515, 100)\n",
    "    words = [PAD_CHAR]\n",
    "    Ws[0][:] = np.zeros(length)\n",
    "    for i, (idx, word) in enumerate(ind2w.items()): # e.g. idx=4, word='aabdominal'\n",
    "        try:\n",
    "            Ws[idx][:] = wv.word_vec(word) # e.g. Ws[1][:] = [ 2.09741426  0.7055248 ...]\n",
    "            words.append(word)\n",
    "        except:\n",
    "            continue\n",
    "    return Ws, words\n",
    "\n",
    "def save_embeddings(Ws, words, out_file):\n",
    "    with open(out_file, 'w', newline='') as f:\n",
    "        for i in range(len(words)):\n",
    "            line = [words[i]]\n",
    "            line.extend([str(d) for d in Ws[i]])\n",
    "            f.write(\" \".join(line) + \"\\n\") # e.g. i=82: abdominal -4.255998134613037 0.5219717621803284 ...\n",
    "\n",
    "# Generate Embeddings by looking up vocab (on notes_labeled) using w2v (on notes_labeled)\n",
    "create_embeddings('%s/%s' % (DATA_PATH, 'w2v.model'), '%s/%s' % (DATA_PATH, 'w2v.embed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 3. DATASET VERIFICATION\n",
    "# 3-1. Codes & Notes\n",
    "#########################\n",
    "# Be able to run all the sections below if all preprocessed data already exists\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "SEED = 1234\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "DATA_PATH = 'mimic-iii-1.4'\n",
    "assert os.path.isdir(DATA_PATH)\n",
    "DATA_FILE = 'notes_labeled.csv'\n",
    "\n",
    "codes = pd.read_csv('%s/%s' % (DATA_PATH, 'codes.csv'), usecols=['HADM_ID', 'ICD9_CODE'], dtype={'HADM_ID': np.uint64, 'ICD9_CODE': str})\n",
    "notes = pd.read_csv('%s/%s' % (DATA_PATH, 'NOTEEVENTS.csv'), usecols=['HADM_ID', 'CATEGORY', 'TEXT'], dtype={'CATEGORY': str, 'TEXT': str})\n",
    "notes_labeled = pd.read_csv('%s/%s' % (DATA_PATH, DATA_FILE),\n",
    "                            usecols=['HADM_ID', 'LENGTH', 'TEXT', 'ICD9_CODE'],\n",
    "                            dtype={'HADM_ID': np.uint64, 'LENGTH': np.uint64, 'TEXT': str, 'ICD9_CODE': str})\n",
    "pcodes = codes[codes.ICD9_CODE.str.find('.')==2]['ICD9_CODE'].tolist() # '.' appears at index2 (decimal point between the 2rd and 3th digit)\n",
    "\n",
    "# 3-1-1.Check: Number of unique ICD9_CODE on codes\n",
    "print('Number of unique ICD9_CODE : ' + str(len(codes['ICD9_CODE'].unique())) +\n",
    "     ' (PROCEDURES_ICD=' + str(len(codes[codes.ICD9_CODE.isin(pcodes)]['ICD9_CODE'].unique())) +\n",
    "     ', DIAGNOSES_ICD=' + str(len(codes[~codes.ICD9_CODE.isin(pcodes)]['ICD9_CODE'].unique())) + ')')\n",
    "\n",
    "# 3-1-2.Check: Number of CATEGORY='Discharge summary' on notes\n",
    "print('Number of unique CATEGORY=Discharge summary : ' + str(len(notes[notes['CATEGORY']=='Discharge summary'])))\n",
    "\n",
    "# 3-1-3.Check: Number of HADM_ID\n",
    "print('Number of unique HADM_IDs : ' + str(len(notes_labeled['HADM_ID'].unique())))\n",
    "\n",
    "# 3-1-4.Check: Labeled Notes\n",
    "print('Some Samples of the notes_labeled :')\n",
    "print(notes_labeled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 3-2. Top Codes & Vocab\n",
    "#########################\n",
    "# 3-2-1.Similar Words\n",
    "def get_similar_words(model, word):\n",
    "    similar_words = model.wv.most_similar(word)\n",
    "    return similar_words\n",
    "\n",
    "codes_top = pd.read_csv('%s/%s' % (DATA_PATH, 'codes_top.csv'), header=None, dtype=str)\n",
    "vocab = pd.read_csv('%s/%s' % (DATA_PATH, 'vocab.csv'), header=None, dtype=str)[0]\n",
    "vocab_list = vocab.tolist()\n",
    "w2v = Word2Vec.load('%s/%s' % (DATA_PATH, 'w2v.model'))\n",
    "\n",
    "# 3-3-1.Check: Top 50 ICD9_CODE\n",
    "print('Top 50 ICD9_CODE :')\n",
    "print(str(codes_top))\n",
    "\n",
    "# 3-3-2.Check: Vocabulary (cosine similarity) on trn\n",
    "print('Vocabulary Size : ' + str(len(vocab_list)))\n",
    "print('Top 20 frequent words :')\n",
    "print(vocab_list[:20])\n",
    "print('\\nBottom 20 frequent words :')\n",
    "print(vocab_list[-20:])\n",
    "print('\\nSimilar words to diabetic :')\n",
    "for word in get_similar_words(w2v, 'diabetic'):\n",
    "    print(\"{0} {1:.3f}\".format(word[0], word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 4. DATASET\n",
    "# 4-1. Custom Dataset\n",
    "#########################\n",
    "# Be able to run all the sections below if all preprocessed data already exists\n",
    "import numpy as np\n",
    "import csv\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "DATA_PATH = 'mimic-iii-1.4'\n",
    "PAD_CHAR = '*'\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filename):        \n",
    "        data = []\n",
    "        with open(filename, \"r\") as file:\n",
    "            csv_reader = csv.DictReader(file, delimiter=',') # the 1st as the fieldnames\n",
    "            for row in csv_reader:\n",
    "                data.append(row)\n",
    "        self.data = data\n",
    "        self.idx2word, self.word2idx = self.load_lookup(f'{DATA_PATH}/vocab.csv', padding=True) # padding=True\n",
    "        self.idx2code, self.code2idx = self.load_lookup(f'{DATA_PATH}/codes_top.csv')\n",
    "    \n",
    "    def load_file(self, filename):\n",
    "        tokens = set()\n",
    "        with open(filename, 'r') as vocabfile:\n",
    "            for i, line in enumerate(vocabfile):\n",
    "                line = line.rstrip()\n",
    "                if line != '':\n",
    "                    tokens.add(line.strip())\n",
    "        return tokens\n",
    "\n",
    "    # lookup for word or code (e.g. idx2token={1:'date'}, token2idx={'date':1})\n",
    "    def load_lookup(self, filename, padding=False):\n",
    "        tokens = self.load_file(filename)\n",
    "        idx2token = {}\n",
    "        if padding:  # padding at index 0\n",
    "            idx2token[0] = PAD_CHAR\n",
    "        for w in sorted(tokens):\n",
    "            idx2token[len(idx2token)] = w\n",
    "        token2idx = {w:i for i, w in idx2token.items()}\n",
    "        return idx2token, token2idx\n",
    "    \n",
    "    # sequence=['date', 'birth', 'name'] -> token2idx={'date':1, 'birth':2} -> [1, 2, 0]\n",
    "    def to_index(self, sequence, token2idx):\n",
    "        ints = []\n",
    "        for word in sequence:\n",
    "            try:\n",
    "                id = token2idx[word]\n",
    "            except:\n",
    "                id = 0 # only for 'TEXT', not 'ICD9_CODE'\n",
    "            ints.append(id)\n",
    "        return ints # e.g. [0, 1, 2]\n",
    "\n",
    "    def to_multi_hot(self, labels):\n",
    "        multi_hot_labels = [0] * len(self.idx2code)\n",
    "        for idx in labels:\n",
    "            multi_hot_labels[idx] = 1\n",
    "        return multi_hot_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        text = data['TEXT'].split()\n",
    "        text = self.to_index(text, self.word2idx)\n",
    "        \n",
    "        labels = data['ICD9_CODE'].split(';')\n",
    "        labels = self.to_index(labels, self.code2idx)\n",
    "        labels = self.to_multi_hot(labels)\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    def get_hadm_id(self, index):\n",
    "        return self.data[index]['HADM_ID']\n",
    "    \n",
    "    def get_text(self, index):\n",
    "        return self.data[index]['TEXT']\n",
    "    \n",
    "    def get_code(self, index):\n",
    "        return self.data[index]['ICD9_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 4-2. Dataloader\n",
    "#########################\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 4-2-1.Collate the list of samples into batches\n",
    "def collate_fn(data):\n",
    "    text, labels = zip(*data)\n",
    "    text = pad_sequence(text, batch_first=True)\n",
    "    labels = torch.stack(labels, 0)   \n",
    "    return text, labels\n",
    "\n",
    "# 4-2-2.Load Embeddings\n",
    "def load_embeddings(embed_file):\n",
    "    embed_file = '%s/%s' % (DATA_PATH, embed_file)\n",
    "    W = []\n",
    "    with open(embed_file) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip().split()\n",
    "            vec = np.array(line[1:]).astype(np.float) # 0.06135951727628708 0.008149753324687481 ... (e.g. index=0:aabdominal)\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6) # normalize the embeddings\n",
    "            W.append(vec)\n",
    "        # Add a vector of unknown gaussian embedding\n",
    "        vec = np.random.randn(len(W[-1])) # len(W[-1]) = 100, e.g. array([-1.49401501, 1.00950034, ...])\n",
    "        vec = vec / float(np.linalg.norm(vec) + 1e-6) # normalize (gaussian) the unknown (random) embedding\n",
    "        W.append(vec)\n",
    "    W = np.array(W)\n",
    "    return W\n",
    "\n",
    "train_set = CustomDataset(f'{DATA_PATH}/train_top.csv')\n",
    "valid_set = CustomDataset(f'{DATA_PATH}/valid_top.csv')\n",
    "test_set = CustomDataset(f'{DATA_PATH}/test_top.csv')\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# 4-2-2.Check: Length of Split TOP Data\n",
    "print('Length of Split TOP Data :')\n",
    "for (name, length) in zip(['train', 'valid', 'test'], [train_set, valid_set, test_set]):\n",
    "    print('{:s} {:d}'.format(name, len(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 5. TRAIN & EVALUATION\n",
    "# 5-1. Evaluation\n",
    "#########################\n",
    "'''\n",
    "# Module Installation\n",
    "!{sys.executable} -m pip install sklearn\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "# Device to Train on\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 5-1-1. Evaluate\n",
    "def eval(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = torch.LongTensor()\n",
    "    y_prob = torch.LongTensor()\n",
    "    y_pred = torch.LongTensor()\n",
    "    for sequences, labels in test_loader:\n",
    "        output = model.forward(sequences)\n",
    "        y_true = torch.cat((y_true, labels.detach().to(device)), dim=0)\n",
    "        y_prob = torch.cat((y_prob, output.detach().to(device)), dim=0)\n",
    "        \n",
    "        o = torch.ones(size=output.shape, dtype=torch.int)\n",
    "        z = torch.zeros(size=output.shape, dtype=torch.int)\n",
    "        y_hat = torch.where(output > 0.5, o, z)\n",
    "        y_pred = torch.cat((y_pred, y_hat.detach().to(device)), dim=0)\n",
    "    \n",
    "    # Precision, Recall, Fb (more general F1-Score), RocAuc\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_prob, average='micro')\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 5-2. Train\n",
    "#########################\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# 5-2-1. Loss Function\n",
    "criterion = nn.BCELoss() # =F.binary_cross_entropy(y_hat, labels)\n",
    "\n",
    "# 5-2-2. Train\n",
    "def train(model, train_loader, test_loader, n_epochs, lr=0.001, model_name='CAML'):\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    print('Model Name: ' + model_name)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            sequences.to(device)\n",
    "            y_hat = model(sequences)\n",
    "            loss = criterion(y_hat, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval(model, test_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r: {:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 6. CNN MODEL\n",
    "# 6-1. CAML\n",
    "#########################\n",
    "from math import floor\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 6-1-1. CAML\n",
    "class CAML(nn.Module):\n",
    "    def __init__(self, Y=50, w2v_model_name='w2v.embed', embed_size=100, num_filter_maps=60, kernel_size=18, dropout=0.2):\n",
    "        super(CAML, self).__init__()\n",
    "        \n",
    "        # Embed layer\n",
    "        W = torch.Tensor(load_embeddings(w2v_model_name))\n",
    "        self.embed = nn.Embedding(num_embeddings=W.size()[0], embedding_dim=W.size()[1], padding_idx=0) # padding_idx=0\n",
    "        self.embed.weight.data = W.clone()\n",
    "        self.embed_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Conv layer (bias=True (default))\n",
    "        self.conv = nn.Conv1d(in_channels=embed_size, out_channels=num_filter_maps, kernel_size=kernel_size, padding=int(floor(kernel_size/2)))\n",
    "        xavier_uniform_(self.conv.weight)\n",
    "\n",
    "        # Context Vectors for Attention\n",
    "        self.U = nn.Linear(in_features=num_filter_maps, out_features=Y)\n",
    "        xavier_uniform_(self.U.weight)\n",
    "\n",
    "        # Final layer: Create a Matrix to use for the L Binary Classifiers\n",
    "        self.final = nn.Linear(in_features=num_filter_maps, out_features=Y)\n",
    "        xavier_uniform_(self.final.weight)\n",
    "        \n",
    "    def forward_embed(self, text):\n",
    "        text = self.embed(text)\n",
    "        text = self.embed_drop(text)\n",
    "        return text\n",
    "        \n",
    "    def forward_conv(self, text):\n",
    "        text = self.conv(text)\n",
    "        text = torch.tanh(text)\n",
    "        return text\n",
    "        \n",
    "    def forward_calc_atten(self, text):\n",
    "        alpha = torch.matmul(self.U.weight, text)\n",
    "        alpha = F.softmax(alpha, dim=2) # = normalized exponential function\n",
    "        return alpha\n",
    "        \n",
    "    def forward_aply_atten(self, alpha, text):\n",
    "        v = torch.matmul(alpha, text)\n",
    "        return v\n",
    "    \n",
    "    def forward_linear(self, v):\n",
    "        m = torch.mul(self.final.weight, v)\n",
    "        s = torch.sum(m, dim=2)\n",
    "        s = s + self.final.bias\n",
    "        y_hat = torch.sigmoid(s)\n",
    "        return y_hat\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # Get Embeddings and Apply Dropout\n",
    "        text = self.forward_embed(text)\n",
    "\n",
    "        # Apply Convolution and Nonlinearity (tanh)\n",
    "        text = text.transpose(1, 2) # (BATCH_SIZE, embed_size, seq_len) <- (BATCH_SIZE, seq_len, embed_size)\n",
    "        text = self.forward_conv(text)\n",
    "        \n",
    "        # Calculate Attention\n",
    "        alpha = self.forward_calc_atten(text)\n",
    "        \n",
    "        # Apply Attention\n",
    "        text = text.transpose(1, 2) # (BATCH_SIZE, seq_len, num_filter_maps) <- (BATCH_SIZE, num_filter_maps, seq_len) \n",
    "        v = self.forward_aply_atten(alpha, text)\n",
    "        \n",
    "        # Final layer Classification\n",
    "        y_hat = self.forward_linear(v)        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 7. RNN MODELs\n",
    "# 7-1. LSTM / GRU\n",
    "#########################\n",
    "# 7-1-1. RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, Y=50, w2v_model_name='w2v.embed', cell_type='lstm', embed_size=100, hidden_size=60, num_layers=1, dropout=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embed layer\n",
    "        W = torch.Tensor(load_embeddings(w2v_model_name))\n",
    "        self.embed = nn.Embedding(num_embeddings=W.size()[0], embedding_dim=W.size()[1], padding_idx=0) # padding_idx=0\n",
    "        self.embed.weight.data = W.clone()\n",
    "        self.embed_drop = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Recurrent layer ('lstm' or 'gru')\n",
    "        self.cell_type = cell_type\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        \n",
    "        # Final layer: Create a Matrix to use for the L Binary Classifiers\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=Y)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward_embed(self, text):\n",
    "        text = self.embed(text)\n",
    "        text = self.embed_drop(text)\n",
    "        return text\n",
    "    \n",
    "    def forward_recur(self, text, hiddens):\n",
    "        output, hiddens = self.rnn(text, hiddens) # last hidden: output[-1], hiddens[0] or hiddens\n",
    "        return hiddens[0] if self.cell_type == 'lstm' else hiddens\n",
    "    \n",
    "    def forward_linear(self, text):\n",
    "        text = self.linear(text)\n",
    "        y_hat = self.activation(text) #.view(BATCH_SIZE, -1)\n",
    "        return y_hat\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # Initialize Hidden\n",
    "        hiddens = self.init_hidden(text.size(0))\n",
    "        \n",
    "        # Get Embeddings and Apply Dropout\n",
    "        text = self.forward_embed(text)\n",
    "\n",
    "        # Apply Recurrent NN\n",
    "        text = text.transpose(0, 1) # (seq_len, BATCH_SIZE, embed_size) <- (BATCH_SIZE, seq_len, embed_size)\n",
    "        hidden = self.forward_recur(text, hiddens) # (num_layers, BATCH_SIZE, hidden_size)\n",
    "        \n",
    "        # Final layer Classification\n",
    "        y_hat = self.forward_linear(hidden[-1]) # (BATCH_SIZE, Y) <- (BATCH_SIZE, hidden_size)\n",
    "        return y_hat\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return (hidden, hidden) if self.cell_type == 'lstm' else hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 8. Model Implementation\n",
    "# 8-1. CAML Model\n",
    "#########################\n",
    "# Model Paths\n",
    "CAML_PATH = '%s/%s' % (DATA_PATH, 'caml.model')\n",
    "LSTM_PATH = '%s/%s' % (DATA_PATH, 'lstm.model')\n",
    "GRU_PATH = '%s/%s' % (DATA_PATH, 'gru.model')\n",
    "\n",
    "# Number of Epochs\n",
    "n_epochs = 10\n",
    "\n",
    "# 8-1-1. Train CNN CAML Model\n",
    "caml = CAML()\n",
    "caml.to(device)\n",
    "train(caml, train_loader, test_loader, n_epochs)\n",
    "torch.save(caml, CAML_PATH)\n",
    "\n",
    "# 8-1-2. Train RNN LSTM Model\n",
    "lstm = RNN()\n",
    "lstm.to(device)\n",
    "train(lstm, train_loader, test_loader, n_epochs, lr=0.01, model_name='LSTM')\n",
    "torch.save(lstm, LSTM_PATH)\n",
    "\n",
    "# 8-1-3. Train RNN GRU Model\n",
    "gru = RNN(cell_type='gru')\n",
    "gru.to(device)\n",
    "train(gru, train_loader, test_loader, n_epochs, lr=0.01, model_name='GRU')\n",
    "torch.save(gru, GRU_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-monitor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# 9. EXAMIATION\n",
    "# 9-1. Model Examination\n",
    "#########################\n",
    "# Be able to run all the sections below if all preprocessed data and learned models already exist\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# 9-1-1. Examine\n",
    "def examine(model, test_loader, test_set, codes_top, filename):\n",
    "    model.eval()\n",
    "    codes_top_sorted = sorted(codes_top)\n",
    "    i, max_acc, min_acc, max_idxs, min_idxs, max_preds, min_preds, max_lengths, min_lengths = -1, 1, 0.8, [], [], [], [], [], []\n",
    "    y_true = torch.LongTensor()\n",
    "    y_pred = torch.LongTensor()\n",
    "    columns = ['HADM_ID', 'ICD9_CODE', 'PRED_CODE', 'PRECISION', 'LENGTH', 'TEXT']\n",
    "    df0 = pd.DataFrame(columns=columns)\n",
    "    counts_tp = Counter()\n",
    "    counts_fp = Counter()\n",
    "    counts_fn = Counter()\n",
    "    for code in codes_top:\n",
    "        counts_tp[code] = 0\n",
    "        counts_fp[code] = 0\n",
    "        counts_fn[code] = 0\n",
    "\n",
    "    for sequences, labels in test_loader:\n",
    "        i += 1\n",
    "        output = model.forward(sequences)\n",
    "        y_true = labels.detach().to(device)\n",
    "        \n",
    "        o = torch.ones(size=output.shape, dtype=torch.int)\n",
    "        z = torch.zeros(size=output.shape, dtype=torch.int)\n",
    "        y_hat = torch.where(output > 0.5, o, z)\n",
    "        y_pred = y_hat.detach().to(device)\n",
    "        \n",
    "        for j, pred in enumerate(y_pred):\n",
    "            # Precision\n",
    "            precision = torch.sum(torch.eq(y_true[j], pred).long()) / len(pred)\n",
    "            idx = i * BATCH_SIZE + j\n",
    "            preds = torch.where(pred > 0)[0]\n",
    "            length = len(torch.where(sequences[j] > 0)[0])\n",
    "            \n",
    "            true_codes = sorted(test_set.get_code(idx).split(';'))\n",
    "            pred_codes = sorted([codes_top_sorted[idx] for idx in preds.tolist()])\n",
    "            df0 = df0.append({'HADM_ID': int(test_set.get_hadm_id(idx)),\n",
    "                            'ICD9_CODE': str(';'.join(true_codes)),\n",
    "                            'PRED_CODE': str(';'.join(pred_codes)),\n",
    "                            'PRECISION': float(precision),\n",
    "                            'LENGTH': int(length),\n",
    "                            'TEXT': str(test_set.get_text(idx))}, ignore_index=True)\n",
    "            \n",
    "            for pred_code in pred_codes:\n",
    "                if pred_code in true_codes:\n",
    "                    counts_tp[pred_code] += 1 # TP\n",
    "                else:\n",
    "                    counts_fp[pred_code] += 1 # FP\n",
    "            for true_code in true_codes:\n",
    "                if true_code not in pred_codes:\n",
    "                    counts_fn[true_code] += 1 # FN\n",
    "    \n",
    "    cols = ['ICD9_CODE', 'TP', 'FP', 'FN']\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    for tp, fp, fn in zip(counts_tp.items(), counts_fp.items(), counts_fn.items()):\n",
    "        df = df.append({'ICD9_CODE': str(tp[0]), 'TP': int(tp[1]), 'FP': int(fp[1]), 'FN': int(fn[1])}, ignore_index=True)\n",
    "\n",
    "    df0.to_csv('%s/%s' % (DATA_PATH, filename + '.csv'), columns=columns, index=False, header=True, quoting=csv.QUOTE_NONE, escapechar='')\n",
    "    df.to_csv('%s/%s' % (DATA_PATH, filename + '_count.csv'), columns=cols, index=False, header=True, quoting=csv.QUOTE_NONE, escapechar='')\n",
    "    return df\n",
    "\n",
    "# Test Dataset\n",
    "codes_top = (pd.read_csv('%s/%s' % (DATA_PATH, 'codes_top.csv'), header=None, dtype=str)[0]).tolist()\n",
    "xticks = np.arange(50)\n",
    "\n",
    "# Model Paths\n",
    "CAML_PATH = '%s/%s' % (DATA_PATH, 'caml.model')\n",
    "LSTM_PATH = '%s/%s' % (DATA_PATH, 'lstm.model')\n",
    "GRU_PATH = '%s/%s' % (DATA_PATH, 'gru.model')\n",
    "\n",
    "# Load Models\n",
    "# 9-1-1. CAML Examination\n",
    "caml = torch.load(CAML_PATH)\n",
    "caml.eval()\n",
    "df_caml = examine(caml, test_loader, test_set, codes_top, 'test_stat_caml')\n",
    "figure(figsize=(15, 3), dpi=80)\n",
    "plt.plot('ICD9_CODE', 'TP', data=df_caml, linewidth=2, color='green', markersize=4, marker='o', markerfacecolor='lime')\n",
    "plt.plot('ICD9_CODE', 'FP', data=df_caml, linewidth=2, color='blue', markersize=4, marker='o', markerfacecolor='skyblue')\n",
    "plt.plot('ICD9_CODE', 'FN', data=df_caml, linewidth=2, color='purple', markersize=4, marker='o', markerfacecolor='plum')\n",
    "plt.xlabel('ICD9_CODE')\n",
    "plt.ylabel('COUNT')\n",
    "plt.xticks(ticks=xticks, rotation=90)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 9-1-2. LSTM Examination\n",
    "lstm = torch.load(LSTM_PATH)\n",
    "lstm.eval()\n",
    "df_lstm = examine(lstm, test_loader, test_set, codes_top, 'test_stat_lstm')\n",
    "figure(figsize=(15, 3), dpi=80)\n",
    "plt.plot('ICD9_CODE', 'TP', data=df_lstm, linewidth=2, color='green', markersize=4, marker='o', markerfacecolor='lime')\n",
    "plt.plot('ICD9_CODE', 'FP', data=df_lstm, linewidth=2, color='blue', markersize=4, marker='o', markerfacecolor='skyblue')\n",
    "plt.plot('ICD9_CODE', 'FN', data=df_lstm, linewidth=2, color='purple', markersize=4, marker='o', markerfacecolor='plum')\n",
    "plt.xlabel('ICD9_CODE')\n",
    "plt.ylabel('COUNT')\n",
    "plt.xticks(ticks=xticks, rotation=90)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 9-1-3. GRU Examination\n",
    "gru = torch.load(GRU_PATH)\n",
    "gru.eval()\n",
    "df_gru = examine(gru, test_loader, test_set, codes_top, 'test_stat_gru')\n",
    "figure(figsize=(15, 3), dpi=80)\n",
    "plt.plot('ICD9_CODE', 'TP', data=df_gru, linewidth=2, color='green', markersize=4, marker='o', markerfacecolor='lime')\n",
    "plt.plot('ICD9_CODE', 'FP', data=df_gru, linewidth=2, color='blue', markersize=4, marker='o', markerfacecolor='skyblue')\n",
    "plt.plot('ICD9_CODE', 'FN', data=df_gru, linewidth=2, color='purple', markersize=4, marker='o', markerfacecolor='plum')\n",
    "plt.xlabel('ICD9_CODE')\n",
    "plt.ylabel('COUNT')\n",
    "plt.xticks(ticks=xticks, rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-invitation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# 10. INVESTIGATION\n",
    "# 10-1. Word Investigation\n",
    "#########################\n",
    "# 10-1-1. Word Investigation\n",
    "def investigate(stat_caml, icd9_code, words):\n",
    "    columns = ['WORD', 'SIMILAR', 'SIMILARITY', 'BOTH', 'PRED', 'ORIG']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for word in words:\n",
    "        list = [(word, 1)] + get_similar_words(w2v, word)\n",
    "        for w in list:\n",
    "            if len(df[df['SIMILAR'] == w[0]]) < 1:\n",
    "                both, orig, pred = 0, 0, 0\n",
    "                df_ = stat_caml[stat_caml['TEXT'].str.contains(w[0])]\n",
    "                for row in df_.itertuples():\n",
    "                    p = icd9_code in str(row.PRED_CODE).split(';')\n",
    "                    o = icd9_code in str(row.ICD9_CODE).split(';')\n",
    "                    if p and o:\n",
    "                        both += 1\n",
    "                    else:\n",
    "                        if p:\n",
    "                            pred += 1\n",
    "                        if o:\n",
    "                            orig += 1\n",
    "                df = df.append({'WORD': str(word), 'SIMILAR': str(w[0]), 'SIMILARITY': str('{0:.3f}'.format(w[1])), 'BOTH': int(both), 'PRED': int(pred), 'ORIG': int(orig)}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# 10-1-2. Load Stat\n",
    "stat_caml = pd.read_csv('%s/%s' % (DATA_PATH, 'test_stat_caml.csv'),\n",
    "                        usecols=['HADM_ID', 'ICD9_CODE', 'PRED_CODE', 'PRECISION', 'LENGTH', 'TEXT'],\n",
    "                        dtype={'HADM_ID': np.uint64, 'ICD9_CODE': str, 'PRED_CODE': str, 'ACC': np.float64, 'LENGTH': np.uint64, 'TEXT': str})\n",
    "\n",
    "# ICD9_CODE = 427.31\n",
    "icd9_code1 = str(427.31)\n",
    "words1 = ['atrial', 'fibrillation', 'heart']\n",
    "print('ICD-9 CODE = ' + icd9_code1 + ' (Atrial Fibrillation related to heart disease)\\n')\n",
    "df1_ = investigate(stat_caml, icd9_code1, words1)\n",
    "print(df1_)\n",
    "\n",
    "df1 = df1_[(df1_['BOTH'] > 0) | (df1_['ORIG'] > 0) | (df1_['PRED'] > 0)]\n",
    "figure(figsize=(10, 2), dpi=80)\n",
    "xticks = np.arange(df1.shape[0])\n",
    "plt.plot('SIMILAR', 'BOTH', data=df1, linewidth=2, color='olivedrab', markersize=4, marker='o', markerfacecolor='greenyellow')\n",
    "plt.plot('SIMILAR', 'PRED', data=df1, linewidth=2, color='teal', markersize=4, marker='o', markerfacecolor='aquamarine')\n",
    "plt.plot('SIMILAR', 'ORIG', data=df1, linewidth=2, color='rebeccapurple', markersize=4, marker='o', markerfacecolor='mediumpurple')\n",
    "plt.xlabel('SIMILAR')\n",
    "plt.ylabel('COUNT')\n",
    "plt.xticks(ticks=xticks, rotation=90)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ICD9_CODE = 99.04\n",
    "icd9_code2 = str(99.04)\n",
    "words2 = ['transfusion', 'packed', 'red', 'blood', 'cell']\n",
    "print('\\nICD-9 CODE = ' + icd9_code2 + ' (Transfusion of Packed Cell, red blood cells that have been separated for blood transfusion)\\n')\n",
    "df2_ = investigate(stat_caml, icd9_code2, words2)\n",
    "print(df2_)\n",
    "\n",
    "df2 = df2_[(df2_['BOTH'] > 0) | (df2_['ORIG'] > 0) | (df2_['PRED'] > 0)]\n",
    "figure(figsize=(10, 2), dpi=80)\n",
    "xticks = np.arange(df2.shape[0])\n",
    "plt.plot('SIMILAR', 'BOTH', data=df2, linewidth=2, color='olivedrab', markersize=4, marker='o', markerfacecolor='greenyellow')\n",
    "plt.plot('SIMILAR', 'PRED', data=df2, linewidth=2, color='teal', markersize=4, marker='o', markerfacecolor='aquamarine')\n",
    "plt.plot('SIMILAR', 'ORIG', data=df2, linewidth=2, color='rebeccapurple', markersize=4, marker='o', markerfacecolor='mediumpurple')\n",
    "plt.xlabel('SIMILAR')\n",
    "plt.ylabel('COUNT')\n",
    "plt.xticks(ticks=xticks, rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 11. DETAIL\n",
    "# 11-1. ICD9_CODE = 427.31\n",
    "#########################\n",
    "# 11-1-1. ICD9_CODE = 427.31\n",
    "text_examples = stat_caml[stat_caml['ICD9_CODE'].str.contains('427.31') &\n",
    "          stat_caml['PRED_CODE'].str.contains('427.31') &\n",
    "          stat_caml['TEXT'].str.contains('atrial') &\n",
    "          stat_caml['TEXT'].str.contains('fibrillation') &\n",
    "          stat_caml['TEXT'].str.contains('heart') &\n",
    "          stat_caml['TEXT'].str.contains('respiratory')]\n",
    "#print(text_examples.sort_values(['LENGTH'], ascending=True))\n",
    "print(text_examples[text_examples['HADM_ID'] == 155451])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-tower",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
